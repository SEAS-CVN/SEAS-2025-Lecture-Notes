\documentclass[11pt]{article}
\usepackage{amsfonts,amsthm,amsmath,amssymb, hyperref, dsfont, enumitem, bbm}
\usepackage{array}
\usepackage{epsfig}
\usepackage{fullpage}
\usepackage{color}
\usepackage{epigraph}
\renewcommand{\epigraphflush}{center}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{titlesec}
\usepackage{ellipsis}
\usepackage{subcaption}
\usepackage[normalem]{ulem}
\usepackage{todonotes}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{clrscode3e,etoolbox}


\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Structure
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Don't put chapter number in section numbers
% \renewcommand*\thesection{\arabic{section}}	% commented out to include chapter number in section numbers

\newcommand{\anchorednote}[2]{ #1 \note{#2} }	% anchored note - needed for latex2edx
\newcommand{\note}[1]{\todo[color=blue!10,
  linecolor=blue!90,size=\small]{\linespread{0.9}\selectfont{#1}\par}}

\usepackage{tcolorbox}
\newtcolorbox{examplebox}{colback=green!5!white}

% \scalebox{1.5}{\begin{tikzpicture}
%   % Put your tikz code here
% \end{tikzpicture}}

\newcommand\question[1]{\vskip0.05in\todo[inline, color=yellow!5]{{\bf
      Study Question:}  #1}} 

\def\myrightmargin{2.0in}

% Make it compact for printing
\renewcommand{\note}[1]{\footnote{#1}}
\def\myrightmargin{1.0in}
\usepackage[left=1in, top=1in, bottom=1in,right=\myrightmargin]{geometry}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Math macros
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use to index over examples

\newcommand\ex[2]{#1^{(#2)}}
% Data sets
\newcommand\data{{\cal D}}
\newcommand\dataTrain{{\cal D}_n}
\newcommand\dataTest{{\cal D}_{n'}}
% Model, hypoth
\newcommand\model{{\cal M}}
\newcommand\hclass{{\cal H}}
% Max likelihood
\newcommand\ml[1]{#1_{\bf ml}}
% Empirical risk min
\newcommand\erm[1]{#1_{\bf erm}}
% Arg max
\newcommand\argmax[1]{{\rm arg}\max_{#1}}
\newcommand\argmin[1]{{\rm arg}\min_{#1}}
% Math
\newcommand{\R}{\mathbb{R}}
% errors
\newcommand{\trainerr}{\mathcal{E}_n}
\newcommand{\testerr}{\mathcal{E}}
% sign
\newcommand{\sign}{\text{sign}}
% 2d vec
\newcommand*{\twodrow}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}
\newcommand*{\twodcol}[2]{\begin{bmatrix} #1 \\ #2 \end{bmatrix}}
% norm














\newcommand{\qn}[1]{\textcolor{orange}{Quynh: #1}}



\begin{document}

\input{allcommands}

\handout{SEAS 2025}{{\bf July 24, 2025}}{Instructor: Quynh Nguyen}{TA: }{Lecture 3: Gradient descent}


\emph{Note}: Based on MIT 6.036 - Intro to Machine Learning (Fall 2019) -- Adapted to SEAS by Nguyen Hoang, Nguyen Huynh, Hoang Nguyen, Quynh Nguyen, Nam Tran.\\

In the previous chapter, 
we showed how to describe an interesting objective function for
machine learning, but we need a way to 
find the optimal $\Theta^* = \argmin{\Theta} J(\Theta)$, particularly
when the objective function is not amenable to analytical
optimization.  For example, this can be the case when $J(\Theta)$
involves a more complex loss function, or more general forms of
regularization.  It can also be the case when there is simply too much
data for analytical inversion of the matrices involved.

There is an
enormous and fascinating literature on the mathematical and algorithmic
foundations of optimization\note{Which you should consider studying
some day!}, but for this class, we will consider one of the simplest
methods, called {\em gradient descent.}  

Intuitively, in one or two dimensions, we can easily think of
$J(\Theta)$ as defining a surface over $\Theta$;  that same idea
extends to higher dimensions.  Now, our objective is to find the
$\Theta$ value at the lowest point on that surface.  One way to think
about gradient descent is that you start at some arbitrary point on
the surface, look to see in which direction the ``hill'' goes down
most steeply, take a small step in that direction, determine the
direction of steepest descent from where you are, take another small
step, etc.  

Below, we explicitly give gradient descent algorithms for one and
multi-dimensional objective functions (sections~\ref{sec:gd_onedim}
and~\ref{sec:gd}).  We then illustrate the application of gradient
descent to a loss function which is not merely mean squared loss
(section~\ref{sec:gd_lrr}).  And we present an important method known
as {\em stochastic gradient descent} (section~\ref{sec:sgd}), which is
especially useful when datasets are too large for descent in a single
batch.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gradient descent in one dimension}

\label{sec:gd_onedim}

We start by considering gradient descent in one dimension. Assume
$\Theta \in \R$, and that we know both $J(\Theta)$ and its first
derivative with respect to $\Theta$, $J'(\Theta)$.  Here is pseudo-code
for gradient descent on an arbitrary function $f$.  Along with $f$ and
its gradient $f'$, we have to specify the initial value for parameter $\Theta$, a
{\em step-size} parameter $\eta$, and an {\em accuracy} parameter~$\epsilon$:

\begin{codebox}
  \Procname{$\proc{1D-Gradient-Descent}(\Theta_{\it init}, \eta, f,
    f', \epsilon)$}
  \li $\Theta^{(0)} \gets \Theta_{\it init}$
  \li $t \gets 0$
  \li \Repeat
  \li   $t \gets t+1$
  \li   $\Theta^{(t)} = \Theta^{(t-1)} - \eta \, f'(\Theta^{(t-1)})$
  \li \Until $|f(\Theta^{(t)}) - f(\Theta^{(t-1)})| < \epsilon$
  \li \Return $\Theta^{(t)}$
\end{codebox}

Note that this algorithm terminates when the change in the function $f$ 
is sufficiently small. There are many other reasonable ways to decide to terminate.
These include the following.
\begin{itemize}
\item Stop after a fixed number of iterations $T$, i.e.\ when $t = T$.
\item Stop when the change in the value of the parameter
$\Theta$ is sufficiently small, i.e.\, when $\left| \Theta^{(t)} - \Theta^{(t-1)} \right|
<\epsilon$.
\item Stop when the derivative $f'$ at the latest value of $\Theta$ is sufficiently small, i.e.\ when $\left|f'(\Theta^{(t)}) \right|
<\epsilon$. 
\end{itemize}

\question{Consider all of the potential stopping criteria for $\proc{1D-Gradient-Descent}$, both in the algorithm as it appears
and listed separately later.
Can you think of ways that any two of the criteria relate to each other?
}

\begin{theorem}
Choose a small distance $\tilde{\epsilon} > 0$. 
If $f$ is sufficiently ``smooth'' and \anchorednote{convex,}{A function is convex if the line segment between any two points on the graph of the function lies above or on the graph.}
and if the step size $\eta$ is sufficiently small, gradient descent will reach a point within $\tilde{\epsilon}$ of a global optimum $\Theta$.

% Previous theorem:
%  for any desired accuracy $\epsilon$, there is some
%  step size $\eta$ such that gradient 
%  descent will converge to within $\epsilon$ of the optimal $\Theta$.
%
% Previous theorem has bug: doesn't quite hold for horizontal line; also, 
% it won't be epsilon away; it'll get arbitrarily close with the distances going to 0
\end{theorem}
However, we must be careful when choosing the step size to prevent
slow convergence, oscillation around the minimum, or divergence.

The following plot illustrates a convex function $f(x) = (x - 2)^2$,
starting gradient descent at $x_{init} = 4.0$ with a step-size of
$1/2$.  It is very well-behaved! 

\begin{center}
\begin{tikzpicture}
\begin{axis}[
    axis lines=middle, 
    xmin=-1, xmax=6,
    ymin=-1, ymax=5,
    xlabel={$x$}, ylabel={$f(x)$},
]
\addplot [domain=-1:6, samples=100] {(x-2)^2}; 
\addplot [only marks,color=black,mark=*,mark size=1.5pt] coordinates {
  (2,0)
  (4,4)};
\draw[-latex,blue,thick] (axis cs: 4,4) -- (axis cs: 2,0);
\end{axis}
\end{tikzpicture}
\end{center}

\question{What happens in this example with very small $\eta$?  With
  very big $\eta$?}

If $f$ is non-convex, where gradient descent converges to depends on
$x_{\it init}$.  First, let's establish some
definitions. Given analytically defined derivatives for $f$, we call a point
where $f'(x) = 0$ and $f''(x)
> 0$ a {\em local minimum} or {\em local
  optimum} of $f$.  More generally, a local minimum is a point that is at
least as low as all the points in some small area around it. A {\em global
minimum} is a point that is at least as low as all the points in the
function. A global minimum is also a local minimum, but a local
minimum does not have to be a global minimum.

If $f$ is non-convex (and sufficiently smooth), 
gradient descent (run long enough with small enough step size) will get very close to a local minimum, but 
we cannot guarantee that it will converge to a global minimum.

The plot below shows two different $x_{\it init}$, and how gradient descent
started from each point heads toward two different local optima.
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    axis lines=middle, 
    xmin=-2, xmax=4,
    ymin=2, ymax=10,
    xlabel={$x$}, ylabel={$f(x)$},
]
\addplot [domain=-2:4, samples=100] {0.5*x^4 - 3*x^3 + 4*x^2 + 3*x + 3}; 
\draw[-latex,blue,thick] (axis cs: -1,7.5) -- (axis cs: -0.52,2.98);
\draw[-latex,blue,thick] (axis cs: -.52,2.98) -- (axis cs: -0.40, 2.65);
\draw[-latex,blue,thick] (axis cs: -0.40, 2.65) -- (axis cs: -0.28, 2.54);

\draw[-latex,blue,thick] (axis cs: 2,9) -- (axis cs: 2.15,8.81);
\draw[-latex,blue,thick] (axis cs: 2.15,8.81) -- (axis cs: 2.38,8.40);
\draw[-latex,blue,thick] (axis cs: 2.38,8.40) -- (axis cs: 2.68, 7.82);
\draw[-latex,blue,thick] (axis cs: 2.68, 7.82) -- (axis cs: 2.93, 7.52);
\draw[-latex,blue,thick] (axis cs: 2.93, 7.52) -- (axis cs: 3, 7.5);

\addplot [only marks,color=black,mark=*,mark size=1.5pt] coordinates {
  (-1, 7.5)
  (-.4, 2.65)
  (2, 9)
  (3, 7.5)};
\end{axis}
\end{tikzpicture}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple dimensions}
\label{sec:gd}

The extension to the case of multi-dimensional $\Theta$ is
straightforward.  Let's  assume $\Theta \in \R^m$, so $f: \R^m
\rightarrow \R$. The 
gradient of $f$ with respect to $\Theta$ is
\[
  \nabla_\Theta f =
  \begin{bmatrix}
    \partial f / \partial \Theta_1 \\
    \vdots \\
    \partial f / \partial \Theta_m
  \end{bmatrix}
\]
The algorithm remains the same, except that the update step in line 5
becomes 
\[ \ex{\Theta}{t} = \ex{\Theta}{t-1} - \eta\nabla_\Theta f(\ex{\Theta}{t-1}) \]
and any termination criteria that depended on the dimensionality
of $\Theta$ would have to change.  The easiest thing is
to keep the test in line 6 as $\left|f(\ex{\Theta}{t}) -
  f(\ex{\Theta}{t-1}) \right| < \epsilon$, which is sensible no matter
the dimensionality of $\Theta$.
\question{Which termination criteria from the 1D case were defined in
a way that assumes $\Theta$ is one dimensional?
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Application to logistic regression objective}

\section{Gradient descent beyond mean square loss}

\label{sec:gd_lrr}

Recall from the previous lecture that choosing a loss function is the
first step in solving a problem using optimization, and for regression
we studied the mean square loss, which captures
loss as $({\rm guess} - {\rm actual})^2$.  The derivative of this,
Eq.() (\qn{REFER TO STATS 3}), was used previously to obtain an
analytical solution to the linear regression problem.  Gradient
descent could also be applied to numerically compute a solution, using
the update rule
\begin{equation}
  \ex{\Theta}{t} = \ex{\Theta}{t-1} - \eta \frac{2}{n} \sum_{i=1}^{n} \left( \left[ \ex{\Theta}{t-1}\right]^T x^{(i)} - y^{(i)} \right) x^{(i)}
\,.
\end{equation}
{~\hfill ~\note{Beware double superscripts!  $\left[ \Theta \right]^T$ is the transpose of the vector $\Theta$}}

But what happens when the models are nonlinear, and the loss function
is more complicated than being merely mean squared error?  Analytical
solutions rarely exist for more complex situations, but gradient
descent can still be applied, especially when the derivatives of the
loss function can be computed analytically.  An important
example is the {\em negative log-likelihood} loss function \note{The loss function $\mathcal{L}_\text{nll}$ is widely employed for solving {\em classification} problems.}
\begin{equation}
  \mathcal{L}_\text{nll}(\ex{g}{i}, \ex{y}{i})  = - {\ex{y}{i}}\log {\ex{g}{i}} - (1 - \ex{y}{i})\log(1 - \ex{g}{i})
\;\;. 
\end{equation}
where the correct values $\ex{y}{i}$ are now assumed to be either $0$
or $1$, and $\ex{g}{i}$ are our guesses.  These guesses are computed
using the model parameters $\Theta = (\theta, \theta_0)$ as
\begin{equation}
  \ex{g}{i} = \sigma(\theta^T \ex{x}{i} + \theta_0)
\,,
\end{equation}
where $\sigma$ is some {\em nonlinear} function which maps $\R$ to
real numbers in the range $0$ to $1$ (inclusive of the endpoints).
Combining this with a \qn{DID WE DEFINE REGULARIZATION IN LEC 2 OR NOT?} \hkn{Not yet} ridge regularization term, we obtain this
``logistic regression'' objective function:
\begin{equation}
  J_\text{lr}(\theta,\theta_0) = \frac{1}{n}\sum_{i=1}^n
                                 \mathcal{L}_\text{nll}
                                 (\ex{g}{i}, \ex{y}{i}) + \frac{\lambda}{2}\norm{\theta}^2
\end{equation}

From a purely pedagogical standpoint, it is useful to write out the
gradient descent algorithm for $J_\text{lr}$, with the goals of
understanding the dimensions of all the quantities involved, and how
to deal with a parameter vector that has both vector and scalar
quantities.  All other considerations, including the context and the
applications of logistic regression, and what particular function to
use for $\sigma$, are deferred to...\qn{already discussed in lec 2?}

First is that we need derivatives with respect to both $\theta_0$ (the scalar component) and $\theta$ (the vector component) of $\Theta$.  Explicitly, they are:
\note{Some passing familiarity with matrix
  derivatives is helpful here.  A foolproof way of computing them is to compute
  partial derivative of $J$ with respect to each component $\theta_i$
  of $\theta$.} 
\begin{align*}
  \nabla_\theta J_\text{lr}(\theta, \theta_0) &=  \frac{1}{n}\sum_{i=1}^n
                    \left(\ex{g}{i} -
                    \ex{y}{i}\right) \ex{x}{i}
                    + \lambda\theta\\
  \frac{\partial J_\text{lr}(\theta, \theta_0)}{\partial \theta_0} &=
                                       \frac{1}{n}\sum_{i=1}^n 
                     \left(\ex{g}{i} -
                    \ex{y}{i} \right) \;\;.
\end{align*}
Note that $\nabla_\theta J_\text{lr}$ will be of shape $d \times 1$ and
$\frac{\partial J_\text{lr}}{\partial \theta_0}$ will be a scalar since
we have separated $\theta_0$ from $\theta$ here.
\question{Convince yourself that the dimensions of all these
  quantities are correct, under the assumption that $\theta$ is $d
  \times 1$. How does $d$ relate to $m$ as discussed for $\Theta$ in the
  previous section?
}

\question{Compute $\nabla_\theta \norm{\theta}^2$ by finding the
vector of partial derivatives $(\partial \norm{\theta}^2 / \partial
\theta_1, \ldots, \partial \norm{\theta}^2 / \partial
\theta_d)$. What is the shape of $\nabla_\theta \norm{\theta}^2$? 
}

\question{Compute $\nabla_\theta \mathcal{L}_\text{nll}(
    \sigma(\theta^T x + \theta_0), y)$ by finding the
    vector of partial derivatives $(\partial \mathcal{L}_\text{nll}(
    \sigma(\theta^T x + \theta_0), y)/ \partial \theta_1, \ldots,
    \partial \mathcal{L}_\text{nll}(
    \sigma(\theta^T x + \theta_0), y) / \partial
\theta_d)$.
}

\question{Use these last two results to verify our derivation above.}

Putting everything together, our gradient descent algorithm for logistic
regression becomes
\begin{codebox}
  \Procname{$\proc{LR-Gradient-Descent}(\theta_{\it init}, \theta_{0
      {\it init}},\eta,\epsilon)$}
  \li $\theta^{(0)} \gets \theta_{\it init}$
  \li $\theta_0^{(0)} \gets \theta_{0 {\it init}}$
  \li $t \gets 0$
  \li \Repeat
  \li   $t \gets t+1$
  \li   $\theta^{(t)} = \theta^{(t-1)} - \eta\left(\frac{1}{n}\sum_{i=1}^n
                    \left(\sigma\left({\ex{\theta}{t-1}}^T \ex{x}{i} + \ex{\theta_0}{t-1}\right) -
                    \ex{y}{i}\right) \ex{x}{i}
                    + \lambda\ex{\theta}{t-1}
                  \right)$
  \li   $\theta_0^{(t)} = \theta_0^{(t-1)} - \eta\left(\frac{1}{n}\sum_{i=1}^n
        \left(\sigma\left({\ex{\theta}{t-1}}^T \ex{x}{i} + \ex{\theta_0}{t-1}\right) -
                    \ex{y}{i} \right) 
        \right)$
        \li \Until $\left| J_{\text{lr}}(\theta^{(t)},\theta_0^{(t)}) - J_{\text{lr}}(\theta^{(t-1)},
        \theta_0^{(t-1)}) \right| <\epsilon$
  \li \Return $\theta^{(t)},\theta_0^{(t)}$
\end{codebox}
\question{Is it okay that $\lambda$ doesn't appear in line 7?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stochastic Gradient Descent}

\label{sec:sgd}

When the form of the gradient is a sum, rather than take one big(ish)
step in the direction of the gradient, we can, instead,
randomly \note{The word ``stochastic'' means probabilistic, or random;
  so does ``aleatoric,'' which is a very cool word.  Look up
  aleatoric music, sometime.} 
 select one term of the sum, and take a very small step in that
direction.  This seems sort of crazy,  but remember that all the
little steps would average out to the same direction as the big step
if you were to stay in one place.   Of course, you're not staying in
that place,  so you move, in expectation, in the direction of the
gradient.  

Most objective functions in machine learning can end up being written
as a sum over data points, in which case, stochastic gradient descent
({\sc sgd}) is implemented by picking a data point randomly out of the
data set, computing the gradient as if there were only that one point
in the data set, and taking a small step  in the negative direction.    

Let's assume our objective has the form
\[
       f(\Theta) = \sum_{i = 1}^n f_i(\Theta)
\;\;,
\]
where $n$ is the number of data points used in the objective (and this
may be different from the number of points available in the whole data
set).  Here is pseudocode for applying {\sc sgd} to such an objective $f$;
it assumes we know the form of $\nabla_\Theta f_i$ for all $i$ in
$1\ldots n$:
\begin{codebox}
  \Procname{$\proc{Stochastic-Gradient-Descent}(\Theta_{\it init}, \eta, f,
    \nabla_\Theta f_1, \ldots, \nabla_\Theta f_n, T)$}
  \li $\Theta^{(0)} \gets \Theta_{\it init}$
  \li \For $t \gets 1$ \To $T$
  \li \Do 
       randomly select $i \in \{1, 2, \dots, n\}$
  \li   $\Theta^{(t)} = \Theta^{(t-1)} - \eta(t) \, \nabla_\Theta f_i(\Theta^{(t-1)})$
      \End
  \li \Return $\Theta^{(t)}$
\end{codebox}

Note that now instead of a fixed  value of $\eta$, $\eta$ is indexed by
the iteration of the algorithm, $t$.
Choosing a good stopping criterion can be a little trickier for {\sc sgd} than
traditional gradient descent. Here we've just chosen
to stop after a fixed number of iterations $T$.

For {\sc sgd} to converge to a local optimum as $t$ increases, the
step size has to decrease as a function of time. The next result shows one
step size sequence that works. 

\begin{theorem}
If $f$ is convex, and $\eta(t)$ is a sequence satisfying
$$ \sum_{t = 1}^{\infty}\eta(t) = \infty \;\;\text{and}\;\;
 \sum_{t = 1}^{\infty}\eta(t)^2 < \infty \;\;,$$
then SGD converges {\em with probability one} 
\note{We have left out some gnarly conditions in this theorem. Also, you
can learn more about the subtle difference between ``with probability
one'' and ``always'' by taking an advanced probability course.}
to the optimal $\Theta$.
\qn{Could someone point me to a proof of this?}
\end{theorem}

One ``legal'' way of setting the step size is to make $\eta(t) = 1/t$ but
people often use rules that decrease more slowly, and so don't
strictly satisfy the criteria for convergence.
\question{
If you start a long way from the optimum, would making $\eta(t)$
decrease more slowly tend to make you move more quickly or more slowly
to the optimum?
}

There are multiple intuitions for why {\sc sgd} might be a better choice
algorithmically than regular {\sc gd} (which is sometimes called {\em
  batch} {\sc gd} ({\sc bgd})):  
\begin{itemize}
\item If your $f$ is actually non-convex, but has many shallow local
  optima that might trap {\sc bgd}, then taking {\em samples} from the
  gradient at some point $\Theta$ might ``bounce'' you around the
  landscape and out of the local optima.
\item Sometimes, optimizing $f$ really well is not what we want to do,
  because it might overfit the training set;  so, in fact, although
  {\sc sgd} might not get lower training error than {\sc bgd}, it
  might result in lower test error.
\item {\sc bgd} typically requires computing some quantity over every
data point in a data set. {\sc sgd} may perform well after visiting only
some of the data. This behavior can be useful for very large data sets --
in runtime and memory savings.
\end{itemize}









\end{document}